{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datset tokenization\n",
    "\n",
    "Tokenization is an important step to represent the input text as numerical inputs. To do so a tokenizer is used. A basic tokenizer could be to split the words.\n",
    "\n",
    "In this case we use the tokenizer used in the GPT2 model of OpenAI and we also train a new one with the corpus of our dataset.\n",
    "\n",
    "The goal is to see if the new tokenizer is capable of tokenizing song chord strings better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Training a tokenizer](https://huggingface.co/learn/nlp-course/en/chapter6/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"lluccardoner/melodyGPT-song-chords-text-1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['genres', 'artist_name', 'song_name', 'chords_str'],\n",
       "    num_rows: 135783\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_name</th>\n",
       "      <th>chords_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>10,000 Hours</td>\n",
       "      <td>G G/B C G G G/B C G G Em C G G Em C G G Em C G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>2 Much</td>\n",
       "      <td>Intro: F#m7 D2 F#m7 D2 F#m7 D2 E F#m7 A/C# E D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>2u (feat. David Guetta)</td>\n",
       "      <td>Em D C C D Em Em D C C D Em Em D C Am D Em G C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>All Around The World</td>\n",
       "      <td>Intro: Em Bm Am C (2x) Em Bm Am C Em Bm Am C ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['canadian pop', 'pop', 'post-teen pop']</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>All Around The World (acoustic)</td>\n",
       "      <td>Intro: Gm - Dm - C - C x2 Gm Dm C C Gm Dm C C ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     genres    artist_name  \\\n",
       "0  ['canadian pop', 'pop', 'post-teen pop']  Justin Bieber   \n",
       "1  ['canadian pop', 'pop', 'post-teen pop']  Justin Bieber   \n",
       "2  ['canadian pop', 'pop', 'post-teen pop']  Justin Bieber   \n",
       "3  ['canadian pop', 'pop', 'post-teen pop']  Justin Bieber   \n",
       "4  ['canadian pop', 'pop', 'post-teen pop']  Justin Bieber   \n",
       "\n",
       "                         song_name  \\\n",
       "0                     10,000 Hours   \n",
       "1                           2 Much   \n",
       "2          2u (feat. David Guetta)   \n",
       "3             All Around The World   \n",
       "4  All Around The World (acoustic)   \n",
       "\n",
       "                                          chords_str  \n",
       "0  G G/B C G G G/B C G G Em C G G Em C G G Em C G...  \n",
       "1  Intro: F#m7 D2 F#m7 D2 F#m7 D2 E F#m7 A/C# E D...  \n",
       "2  Em D C C D Em Em D C C D Em Em D C Am D Em G C...  \n",
       "3   Intro: Em Bm Am C (2x) Em Bm Am C Em Bm Am C ...  \n",
       "4  Intro: Gm - Dm - C - C x2 Gm Dm C C Gm Dm C C ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\t \\t\\tIntro : D D Dm D B|------10----10--12^13-12--10----10---10--12--10-------10--| G|-9/11----11--------------------------------------9/11-----| D|----------------------------------------------------------| A|----------------------------------------------------------| E|----------------------------------------------------------| B|------10---------------------| G|-9/11----11-9-9/11-11\\\\7-7/9--| D|-----------------------------| A|-----------------------------| E|-----------------------------| D\\t Bm \\tBb\\t D \\t\\t\\tBm \\t Bb\\t\\tD \\t\\t\\tBm Bb\\t\\t\\tD \\t\\t\\t Bm Bb\\t\\t\\tD A\\t Bm F#m\\t G A\\t Bm F#m\\t G A\\t Bm F#m\\t\\tD A\\t Bm F#m\\t\\tD Intro : D D Dm D B|------10----10--12^13-12--10----10---10--12--10-------10--| G|-9/11----11--------------------------------------9/11-----| D|----------------------------------------------------------| A|----------------------------------------------------------| E|----------------------------------------------------------| B|------10---------------------| G|-9/11----11-9-9/11-11\\\\7-7/9--| D|-----------------------------| A|-----------------------------| E|-----------------------------| D\\t Bm \\tBb\\t D D\\t Bm \\tBb\\t D D\\t \\t\\t Bm \\tBb\\t D D\\t \\t\\t\\t Bm \\tBb\\t D A\\t Bm F#m\\t G A\\t Bm F#m\\t G A\\t Bm F#m\\t\\tD A\\t Bm F#m\\t\\tD Ponte (Guitarra 1): G A G A A\\t Bm F#m\\t G A\\t Bm F#m\\t G A\\t Bm F#m\\t\\tD A\\t Bm F#m\\t\\tD '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = df.sample(1)\n",
    "example_chords = example[\"chords_str\"].iloc[0]\n",
    "example_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chords = \"Intro: Adim G7/13 Em Bb (4x) G#dim Bm/C F#m Ab|---------------------------------| (Bridge) C G Em7 Asus4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 tokenizer has vocab size of 50000 + 256 + 1 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(gpt2_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the original GPT2 tokenizer does a great job on separating the chords and the chords alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Int', 'ro', ':', 'ĠAd', 'im', 'ĠG', '7', '/', '13', 'ĠEm', 'ĠB', 'b', 'Ġ(', '4', 'x', ')', 'ĠG', '#', 'dim', 'ĠB', 'm', '/', 'C', 'ĠF', '#', 'm', 'ĠAb', '|', '--------------------------------', '-|', 'Ġ(', 'Bridge', ')', 'ĠC', 'ĠG', 'ĠEm', '7', 'ĠAsus', '4']\n"
     ]
    }
   ],
   "source": [
    "tokens = gpt2_tokenizer.tokenize(example_chords)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5317, 305, 25, 1215, 320, 402, 22, 14, 1485, 2295, 347, 65, 357, 19, 87, 8, 402, 2, 27740, 347, 76, 14, 34, 376, 2, 76, 2275, 91, 3880, 22831, 357, 37385, 8, 327, 402, 2295, 22, 46301, 19]\n"
     ]
    }
   ],
   "source": [
    "ids = gpt2_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(step: int = 1000):\n",
    "    return (\n",
    "        df[\"chords_str\"][i : i + step].values.tolist()\n",
    "        for i in range(0, len(df[\"chords_str\"]), step)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chords_gpt2_tokenizer = gpt2_tokenizer.train_new_from_iterator(training_corpus, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see cases where this tokenizers performs a little bit better. For example the diminished chords are not splitted into two tokens:\n",
    "* Original GPT2 tokenizer: \"Gdim\" -> [\"Gd\", \"im\"]\n",
    "* New GTP2 tokenizer: \"Gdim\" -> [\"Gdim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Intro', ':', 'ĠAdim', 'ĠG', '7', '/', '13', 'ĠEm', 'ĠBb', 'Ġ(', '4', 'x', ')', 'ĠG', '#', 'dim', 'ĠBm', '/', 'C', 'ĠF', '#', 'm', 'ĠAb', '|---------------------------------|', 'Ġ(', 'Bridge', ')', 'ĠC', 'ĠG', 'ĠEm', '7', 'ĠAsus', '4']\n"
     ]
    }
   ],
   "source": [
    "new_tokens = chords_gpt2_tokenizer.tokenize(example_chords)\n",
    "print(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287, 26, 659, 260, 23, 15, 303, 268, 272, 279, 20, 88, 9, 260, 3, 294, 270, 15, 35, 264, 3, 77, 284, 613, 279, 719, 9, 262, 260, 268, 23, 319, 20]\n"
     ]
    }
   ],
   "source": [
    "new_ids = chords_gpt2_tokenizer.convert_tokens_to_ids(new_tokens)\n",
    "print(new_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save new trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chords_gpt2_tokenizer.push_to_hub(\"lluccardoner/melodyGPT-song-chords-tokenizer-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19972\n"
     ]
    }
   ],
   "source": [
    "t = AutoTokenizer.from_pretrained(\"lluccardoner/melodyGPT-song-chords-tokenizer-1\")\n",
    "print(t.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
